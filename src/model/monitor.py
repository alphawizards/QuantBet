"""
Model Monitoring and Drift Detection Module.

Tracks prediction performance and feature distribution changes
in production. Alerts when model degradation is detected.

Implements SKILL.md patterns:
    - Observability: Monitor everything critical
    - Model monitoring and drift detection
    - Proactive alerting

Monitors:
    - Prediction accuracy (Brier score rolling)
    - Calibration error by probability bin
    - Feature distribution shifts
    - ROI vs expected
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Tuple, Callable
import json
import logging
import numpy as np
import pandas as pd
from pathlib import Path

logger = logging.getLogger(__name__)


class AlertSeverity(Enum):
    """Severity levels for monitoring alerts."""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


@dataclass
class MonitoringAlert:
    """Alert generated by monitoring system."""
    severity: AlertSeverity
    metric: str
    message: str
    current_value: float
    threshold: float
    timestamp: datetime = field(default_factory=datetime.now)
    
    def __str__(self) -> str:
        icon = {
            AlertSeverity.INFO: "â„¹ï¸",
            AlertSeverity.WARNING: "âš ï¸",
            AlertSeverity.CRITICAL: "ðŸš¨"
        }[self.severity]
        
        return f"{icon} [{self.severity.value.upper()}] {self.metric}: {self.message}"


@dataclass
class PredictionLog:
    """Log entry for a single prediction."""
    game_id: str
    predicted_prob: float
    actual_outcome: Optional[bool]
    odds: float
    stake: float
    profit: float
    timestamp: datetime
    features: Dict[str, float] = field(default_factory=dict)


@dataclass
class DriftReport:
    """Report on feature or prediction drift."""
    metric: str
    baseline_mean: float
    current_mean: float
    drift_magnitude: float
    is_significant: bool
    p_value: float
    
    @property
    def drift_direction(self) -> str:
        if self.drift_magnitude > 0:
            return "increasing"
        elif self.drift_magnitude < 0:
            return "decreasing"
        return "stable"


@dataclass
class CalibrationReport:
    """Report on prediction calibration."""
    bins: List[Tuple[float, float]]
    predicted_probs: List[float]
    actual_rates: List[float]
    sample_counts: List[int]
    calibration_error: float
    max_calibration_error: float
    
    def __str__(self) -> str:
        lines = ["Calibration Report", "=" * 40]
        lines.append(f"ECE: {self.calibration_error:.4f}")
        lines.append(f"MCE: {self.max_calibration_error:.4f}")
        lines.append("\nBin | Predicted | Actual | Count")
        lines.append("-" * 40)
        
        for i, (lo, hi) in enumerate(self.bins):
            pred = self.predicted_probs[i]
            actual = self.actual_rates[i]
            count = self.sample_counts[i]
            lines.append(f"{lo:.1f}-{hi:.1f} | {pred:.3f} | {actual:.3f} | {count}")
        
        return "\n".join(lines)


class ModelMonitor:
    """
    Production model monitoring system.
    
    Tracks:
        - Rolling prediction accuracy
        - Calibration by probability bin
        - Feature distribution changes
        - Betting performance (ROI, drawdown)
    
    Example:
        >>> monitor = ModelMonitor()
        >>> monitor.log_prediction(pred, actual=True, odds=2.0, stake=10)
        >>> if monitor.has_alerts():
        ...     for alert in monitor.get_alerts():
        ...         send_notification(alert)
    """
    
    # Default thresholds
    BRIER_THRESHOLD = 0.27  # Alert if above this
    CALIBRATION_THRESHOLD = 0.10  # Max calibration error
    ROI_THRESHOLD = -0.10  # Alert if ROI below this
    DRIFT_THRESHOLD = 0.2  # Feature drift threshold
    
    def __init__(
        self,
        storage_path: Optional[str] = None,
        rolling_window: int = 30,
        alert_callbacks: Optional[List[Callable]] = None
    ):
        """
        Initialize model monitor.
        
        Args:
            storage_path: Path to store logs (optional)
            rolling_window: Number of predictions for rolling metrics
            alert_callbacks: Functions to call when alerts generated
        """
        self._storage_path = Path(storage_path) if storage_path else None
        self._rolling_window = rolling_window
        self._alert_callbacks = alert_callbacks or []
        
        self._predictions: List[PredictionLog] = []
        self._alerts: List[MonitoringAlert] = []
        self._baseline_features: Optional[pd.DataFrame] = None
        
        # Custom thresholds
        self.thresholds = {
            'brier_score': self.BRIER_THRESHOLD,
            'calibration_error': self.CALIBRATION_THRESHOLD,
            'roi': self.ROI_THRESHOLD,
            'feature_drift': self.DRIFT_THRESHOLD,
        }
        
        if self._storage_path:
            self._storage_path.mkdir(parents=True, exist_ok=True)
    
    def log_prediction(
        self,
        game_id: str,
        predicted_prob: float,
        actual_outcome: Optional[bool],
        odds: float,
        stake: float = 0.0,
        features: Optional[Dict[str, float]] = None
    ) -> None:
        """
        Log a prediction and its outcome.
        
        Args:
            game_id: Unique game identifier
            predicted_prob: Model's predicted probability
            actual_outcome: True if prediction was correct (None if pending)
            odds: Decimal odds for the bet
            stake: Amount staked
            features: Feature values used for prediction
        """
        # Calculate profit
        if actual_outcome is None:
            profit = 0.0
        elif actual_outcome:
            profit = stake * (odds - 1)
        else:
            profit = -stake
        
        log_entry = PredictionLog(
            game_id=game_id,
            predicted_prob=predicted_prob,
            actual_outcome=actual_outcome,
            odds=odds,
            stake=stake,
            profit=profit,
            timestamp=datetime.now(),
            features=features or {}
        )
        
        self._predictions.append(log_entry)
        
        # Check for alerts if we have resolved predictions
        if actual_outcome is not None:
            self._check_alerts()
        
        # Persist to storage
        if self._storage_path:
            self._save_log_entry(log_entry)
        
        logger.debug(f"Logged prediction for {game_id}: p={predicted_prob:.3f}")
    
    def update_outcome(
        self,
        game_id: str,
        actual_outcome: bool
    ) -> None:
        """Update outcome for a pending prediction."""
        for pred in self._predictions:
            if pred.game_id == game_id and pred.actual_outcome is None:
                pred.actual_outcome = actual_outcome
                if actual_outcome:
                    pred.profit = pred.stake * (pred.odds - 1)
                else:
                    pred.profit = -pred.stake
                
                self._check_alerts()
                return
        
        logger.warning(f"Prediction not found for game_id: {game_id}")
    
    def _check_alerts(self) -> None:
        """Check metrics and generate alerts if thresholds exceeded."""
        resolved = [p for p in self._predictions if p.actual_outcome is not None]
        
        if len(resolved) < 10:
            return  # Not enough data
        
        recent = resolved[-self._rolling_window:]
        
        # Check Brier score
        brier = self._calculate_brier_score(recent)
        if brier > self.thresholds['brier_score']:
            self._add_alert(
                AlertSeverity.WARNING,
                "brier_score",
                f"Brier score {brier:.4f} exceeds threshold",
                brier,
                self.thresholds['brier_score']
            )
        
        # Check calibration
        cal_report = self.check_calibration(recent)
        if cal_report.calibration_error > self.thresholds['calibration_error']:
            self._add_alert(
                AlertSeverity.WARNING,
                "calibration_error",
                f"Calibration error {cal_report.calibration_error:.4f} exceeds threshold",
                cal_report.calibration_error,
                self.thresholds['calibration_error']
            )
        
        # Check ROI
        total_staked = sum(p.stake for p in recent if p.stake > 0)
        total_profit = sum(p.profit for p in recent)
        roi = total_profit / total_staked if total_staked > 0 else 0
        
        if roi < self.thresholds['roi']:
            self._add_alert(
                AlertSeverity.CRITICAL,
                "roi",
                f"ROI {roi:.2%} below threshold",
                roi,
                self.thresholds['roi']
            )
    
    def _add_alert(
        self,
        severity: AlertSeverity,
        metric: str,
        message: str,
        current: float,
        threshold: float
    ) -> None:
        """Add alert and trigger callbacks."""
        alert = MonitoringAlert(
            severity=severity,
            metric=metric,
            message=message,
            current_value=current,
            threshold=threshold
        )
        
        self._alerts.append(alert)
        logger.warning(str(alert))
        
        # Trigger callbacks
        for callback in self._alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                logger.error(f"Alert callback failed: {e}")
    
    def _calculate_brier_score(
        self,
        predictions: List[PredictionLog]
    ) -> float:
        """Calculate Brier score for predictions."""
        if not predictions:
            return 0.0
        
        squared_errors = []
        for p in predictions:
            if p.actual_outcome is not None:
                outcome = 1.0 if p.actual_outcome else 0.0
                squared_errors.append((p.predicted_prob - outcome) ** 2)
        
        return float(np.mean(squared_errors)) if squared_errors else 0.0
    
    def check_calibration(
        self,
        predictions: Optional[List[PredictionLog]] = None,
        n_bins: int = 10
    ) -> CalibrationReport:
        """
        Check prediction calibration by probability bin.
        
        Calibration = how close predicted probabilities are to
        actual outcome rates.
        """
        if predictions is None:
            predictions = [p for p in self._predictions if p.actual_outcome is not None]
        
        if len(predictions) == 0:
            return CalibrationReport(
                bins=[], predicted_probs=[], actual_rates=[],
                sample_counts=[], calibration_error=0.0, max_calibration_error=0.0
            )
        
        # Create bins
        bin_edges = np.linspace(0, 1, n_bins + 1)
        bins = list(zip(bin_edges[:-1], bin_edges[1:]))
        
        predicted_probs = []
        actual_rates = []
        sample_counts = []
        
        for lo, hi in bins:
            bin_preds = [
                p for p in predictions 
                if lo <= p.predicted_prob < hi or (hi == 1.0 and p.predicted_prob == 1.0)
            ]
            
            if len(bin_preds) == 0:
                predicted_probs.append((lo + hi) / 2)
                actual_rates.append(0.0)
                sample_counts.append(0)
            else:
                avg_pred = np.mean([p.predicted_prob for p in bin_preds])
                avg_actual = np.mean([1.0 if p.actual_outcome else 0.0 for p in bin_preds])
                
                predicted_probs.append(float(avg_pred))
                actual_rates.append(float(avg_actual))
                sample_counts.append(len(bin_preds))
        
        # Calculate ECE (Expected Calibration Error)
        total = sum(sample_counts)
        if total > 0:
            ece = sum(
                count * abs(pred - actual) 
                for pred, actual, count in zip(predicted_probs, actual_rates, sample_counts)
            ) / total
        else:
            ece = 0.0
        
        # Max calibration error
        mce = max(
            abs(pred - actual) 
            for pred, actual in zip(predicted_probs, actual_rates)
        ) if predicted_probs else 0.0
        
        return CalibrationReport(
            bins=bins,
            predicted_probs=predicted_probs,
            actual_rates=actual_rates,
            sample_counts=sample_counts,
            calibration_error=ece,
            max_calibration_error=mce
        )
    
    def calculate_drift(
        self,
        feature_name: str,
        window: int = 30
    ) -> Optional[DriftReport]:
        """
        Calculate drift for a specific feature.
        
        Compares recent feature distribution to baseline.
        """
        if self._baseline_features is None or feature_name not in self._baseline_features.columns:
            return None
        
        recent = [
            p.features.get(feature_name) 
            for p in self._predictions[-window:]
            if feature_name in p.features
        ]
        
        if len(recent) < 10:
            return None
        
        baseline_mean = self._baseline_features[feature_name].mean()
        baseline_std = self._baseline_features[feature_name].std()
        current_mean = np.mean(recent)
        
        # Z-score based drift
        if baseline_std > 0:
            drift_magnitude = (current_mean - baseline_mean) / baseline_std
        else:
            drift_magnitude = current_mean - baseline_mean
        
        # Simple significance check
        is_significant = abs(drift_magnitude) > self.thresholds['feature_drift']
        
        return DriftReport(
            metric=feature_name,
            baseline_mean=float(baseline_mean),
            current_mean=float(current_mean),
            drift_magnitude=float(drift_magnitude),
            is_significant=is_significant,
            p_value=0.0  # Would need proper statistical test
        )
    
    def set_baseline(self, features: pd.DataFrame) -> None:
        """Set baseline feature distributions for drift detection."""
        self._baseline_features = features.copy()
        logger.info(f"Set baseline with {len(features)} samples, {len(features.columns)} features")
    
    def get_alerts(
        self,
        severity: Optional[AlertSeverity] = None,
        since: Optional[datetime] = None
    ) -> List[MonitoringAlert]:
        """Get alerts, optionally filtered."""
        alerts = self._alerts
        
        if severity:
            alerts = [a for a in alerts if a.severity == severity]
        
        if since:
            alerts = [a for a in alerts if a.timestamp >= since]
        
        return alerts
    
    def has_alerts(self, severity: Optional[AlertSeverity] = None) -> bool:
        """Check if there are any alerts."""
        return len(self.get_alerts(severity)) > 0
    
    def clear_alerts(self) -> None:
        """Clear all alerts."""
        self._alerts.clear()
    
    def get_summary(self) -> Dict:
        """Get monitoring summary."""
        resolved = [p for p in self._predictions if p.actual_outcome is not None]
        
        total_staked = sum(p.stake for p in resolved if p.stake > 0)
        total_profit = sum(p.profit for p in resolved)
        
        return {
            'total_predictions': len(self._predictions),
            'resolved_predictions': len(resolved),
            'pending_predictions': len(self._predictions) - len(resolved),
            'brier_score': self._calculate_brier_score(resolved),
            'total_staked': total_staked,
            'total_profit': total_profit,
            'roi': total_profit / total_staked if total_staked > 0 else 0,
            'alerts_count': len(self._alerts),
            'critical_alerts': len([a for a in self._alerts if a.severity == AlertSeverity.CRITICAL]),
        }
    
    def _save_log_entry(self, log: PredictionLog) -> None:
        """Persist log entry to storage."""
        if not self._storage_path:
            return
        
        log_file = self._storage_path / "prediction_log.jsonl"
        
        entry = {
            'game_id': log.game_id,
            'predicted_prob': log.predicted_prob,
            'actual_outcome': log.actual_outcome,
            'odds': log.odds,
            'stake': log.stake,
            'profit': log.profit,
            'timestamp': log.timestamp.isoformat(),
            'features': log.features,
        }
        
        with open(log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')
